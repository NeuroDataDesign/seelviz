{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Techniques Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "K-Means clustering is a regression technique which involves 'fitting' a number 'n' of given values from a dataset around a pre-defined number of 'k' clusters.  The K-means clustering process seeks to minimize the Euclidean distance from each point 'n' from its associated 'k' cluster.  Note that although the 'k' clusters inhabit the same coordinate space as the 'n' data points, the 'k' cluster locations are not necessarily derived from the set of 'n' points.\n",
    "\n",
    "The process of K-means involves first defining the number of 'k' clusters.  Defining the number of 'k' means is a non-trivial problem that is dependent on the specific experiment at hand.  Analysis with an elbow plot showing explained variance as a function of number of clusers allows the designer to designate the 'k' number by choosing the point where additional 'k' clusters has diminishing returns for variance. \n",
    "\n",
    "\n",
    "The actual algorithm behind K-means clustering is relatively simple.  Given 'n' data points and 'k' clusters, we're looking for µ<sub>k</sub> (cluster means that minimize the Euclidean distance between the 'n' data points and said cluster).  Effectively, the objective function we're trying to minimize can be written as (courtesy SciKit):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum\\limits_{i=0}^n \\min_{µ_j \\subseteq C} ||(x_j - µ_i)||^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the analysis, we must first choose 'k' starting mean µ's.  This is often achieved by randomly sampling 'k' points from the dataset  arbitrarily.  Below, we have an image of the data set at the start, with arbitrarily placed means. *(Image sequence courtesy David Runyan, from Project Rhea)*\n",
    "\n",
    "![Image of Start](clusteringassets/kmeans1.png)\n",
    "\n",
    "After choosing the initial set of starting means, we can apply a three step iterative process to eventually 'converge' on finalized means:\n",
    "\n",
    "1)  **Fit n points to µ means:**   Calculate the Euclidean distances and assign each 'n' to its closest µ.  In this case, all points are closest to the blue node, so all 'n' are assigned to the blue 'µ'.\n",
    "\n",
    "![Image after first assignment](clusteringassets/kmeans2.png)\n",
    "\n",
    "2)  **Define new µ's by finding average:**   Find the center of each cluster and assign those as new µ's.  Only the blue 'µ' moves, as the other nodes have no values assigned to them.\n",
    "\n",
    "![Image after first recalculation](clusteringassets/kmeans3.png)\n",
    "\n",
    "3)  **Repeat until convergence:**   Repeat until there is no change in the µ positions.  I've shown the final converged image below.\n",
    "\n",
    "![Image after convergence](clusteringassets/kmeans14.png)\n",
    "\n",
    "**PROS:**\n",
    "1.  Oft-implemented, existing documentation.\n",
    "2.  Has SciKit implementation in Python.\n",
    "\n",
    "**CONS:**\n",
    "1.  Defining number of means is a non-trivial process.\n",
    "2.  Does not necessarily converge to global solution, as randomized starting positions will alter final result for different results.\n",
    "3.  Does not set the means to positions to some value from the dataset (if we're interested in brain clusters, we want our clusters to be centered around some set of pre-defined nodes, not some weighted average determined by k-means.)\n",
    "4.  Assumes similarly sized 'clusters', which is a very large assumption to be making for brain areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Medoid Clustering\n",
    "\n",
    "K-Medoids clustering is similar to K-means in that both start with a known, pre-defined number of 'k' clusters.  However, K-medoids sets the 'k' clusters to points that occur naturally in the dataset of 'n' points.  Thus, while K-means finds new 'means' and fits each 'n' point to these new means by reducing some summed error, K-medoids instead seeks to maximize some measure of similarity (eg: the similarity matrix) between each points and its medoid (which itself is one of the pre-existing points).\n",
    "\n",
    "The process for K-Medoids is similar to K-Means; again, the problem of defining the number of 'k' means is a difficult process.  From there, an additional difficulty lies in how to define the 'similarity' between two points.  A way to do so would be to make a similarity matrix that is has each corresponding value as the inverse of the Euclidean distance between the points.  Aside from this, a similar process would be applied iteratively to converge to medoids that maximize the similarity between their respective nodes.\n",
    "\n",
    "The specific name of the algorithmic approach is Partitioning Around Medoids, or PAM.  The steps are, specifically:\n",
    "1)  Arbitrarily select 'k' of the 'n' nodes as the medoids.\n",
    "2)  Calculate the total 'similarity' (eg, by finding the inverse of the total of the distances between all 'n' and their closest 'k', or by using some other measure).\n",
    "3)  Swap one 'n' with one 'k', and recalculate the 'similarity' measure.  If the 'similarity' increased, keep the new configuration and continue.  Otherwise, return to the previous configuration.\n",
    "\n",
    "**PROS:**\n",
    "1.  Simple, like K-means.\n",
    "\n",
    "**CONS:**\n",
    "1.  Has similiar faults as the K-means methodology (defining 'k', not necessarily global, assumes equal sized regions)\n",
    "2.  Recalculating similarity at each step relative to all other points is very computationally intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "\\nabla \\times \\vec{\\mathbf{B}} -\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{E}}}{\\partial t} & = \\frac{4\\pi}{c}\\vec{\\mathbf{j}} \\\\\n",
    "\\nabla \\cdot \\vec{\\mathbf{E}} & = 4 \\pi \\rho \\\\\n",
    "\\nabla \\times \\vec{\\mathbf{E}}\\, +\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{B}}}{\\partial t} & = \\vec{\\mathbf{0}} \\\\\n",
    "\\nabla \\cdot \\vec{\\mathbf{B}} & = 0\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
