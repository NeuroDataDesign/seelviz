{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ilastik for 10/31/16 Week\n",
    "From last week, I was able to generate 3D TIFF slices and image classifiers on Fear199 downsampled data.  However, my problems were that:  \n",
    "1)  The TIFF slices were odd, cigar-shaped tubes.  \n",
    "2)  I was unable to generate a significant classifier using the existing data because of the weird image layout.  \n",
    "\n",
    "What I did this week was:  \n",
    "1)  Figure out why my original data was the odd cigar-shaped data.    \n",
    "2)  Correctly generate a subset of TIFF slices for Fear199.   \n",
    "3)  Generate a pixel-based object classifier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Why was my original data cigar-shaped?  \n",
    "When downloading the image from ndreg, there were two different approaches to generating the numpy array.  I've shown both below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Script used to download nii run on Docker\n",
    "from ndreg import *\n",
    "import matplotlib\n",
    "import ndio.remote.neurodata as neurodata\n",
    "import nibabel as nb\n",
    "inToken = \"Fear199\"\n",
    "nd = neurodata()\n",
    "print(nd.get_metadata(inToken)['dataset']['voxelres'].keys())\n",
    "inImg = imgDownload(inToken, resolution=5)\n",
    "imgWrite(inImg, \"./Fear199.nii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method 1:\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import scipy.misc\n",
    "TokenName = 'Fear199.nii'\n",
    "img = nib.load(TokenName)\n",
    "\n",
    "## Convert into np array (or memmap in this case)\n",
    "data = img.get_data()\n",
    "print data.shape\n",
    "print type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Method 2:\n",
    "rawData = sitk.GetArrayFromImage(inImg)  ## convert to simpleITK image to normal numpy ndarray\n",
    "print type(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, method 1 generates an array with shape (x, y, z) -- specifically, (540, 717, 1358).  The method 2 generates a numpy array with shape (z, y, x) -- specifically, (1358, 717, 540).  Since we want the first column to be z slices, the original method was granting me x-slices (hence the cigar-tube dimensions).\n",
    "\n",
    "In order to interconvert, we can either just use the rawData approach after directly calling from ndstore, or we can take our numpy array after loading from nibabel and use numpy's swapaxes method to just swap two of the dimensions (shown below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## if we have (i, j, k), we want (k, j, i)  (converts nibabel format to sitk format)\n",
    "new_im = newer_img.swapaxes(0,2) # just swap i and k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Generating raw TIFF slices.\n",
    "Now that I have appropiate coordinates, I generated a subset of TIFF slices to run the training module for the image classifier.  Using the script here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plane = 0;\n",
    "for plane in (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 100, 101, 102, 103, 104):\n",
    "    output = np.asarray(rawData[plane])\n",
    "    ## Save as TIFF for Ilastik\n",
    "    scipy.misc.toimage(output).save('RAWoutfile' + TokenName + 'ITK' + str(plane) + '.tiff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, I generated data for the first 13 planes, and then some subset of planes from z = 100 to z = 104.  I then trained the classifier on the 0 through 12 data, and used the 100 to 104 slices to validate my results.  Below I've included images of the raw and histogram equalized images at one specific slice to just show the raw images I was working with.\n",
    "\n",
    "**Raw Fear 199 at Z = 100:**\n",
    "![rawFear199Slice100](images/oct31/RAWoutfileFear199.niiITK100.png)\n",
    "\n",
    "**Histogram Equalized Fear 199 at Z = 100:**\n",
    "![histeqFear199Slice100](images/oct31/HISTEQoutfileFear199ITK100.png)\n",
    "\n",
    "In order to generate the histogram equalized TIFF slices, I wrote a Jupyter notebook here:\n",
    "https://neurodatadesign.github.io/seelviz/tony/html/generate+histEQ.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Generate a pixel-based object-classifier for both the raw and the histogram equalized data.\n",
    "From there, I generated a pixel-based object-classifier for both the raw and the histogram equalized data.  Shown first are images demonstrating me generating the classifier for the raw data.  Then I show the images of me generating the classifier for the histogram equalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the raw data classifier.  I load in planes 0-12 TIFF slices and generate a classifier by training it to select for borders, background, and individual bright points:\n",
    "\n",
    "**Loading Raw Training Data**\n",
    "![loadingrawFear199Training](images/oct31/loadingrawfear199.png)\n",
    "\n",
    "**Selecting Borders in Raw Training Data**\n",
    "![trainingbordersFear199](images/oct31/trainingonbordersrawfear199.png)\n",
    "\n",
    "**Selecting Bright Points in Raw Training Data**\n",
    "![trainingbrightpointsFear199](images/oct31/trainingonbrightpointsrawfear199.png)\n",
    "\n",
    "**Closeup of Feature Selection in Raw Training Data**\n",
    "![closeupfeatureselectionrawfear199](images/oct31/closeupfeatureselectionrawfear199.png)\n",
    "\n",
    "**Object Output for Raw Training Data**\n",
    "![exampleobjectsrawfear199](images/oct31/exampleobjectsrawfear199.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run Ilastik headlessly on 3D classified data, use globstring syntax to tell ilastik which images to combine for each volume.  EG:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./run_ilastik.sh --headless --project=MyProject.ilp \"my_next_stack_*.png\" \"my_other_stack_*.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me to get it to work, I made a folder called \"runtime\" where each file was named WITHOUT the underscores.  Make sure to add the * to get it to work, also."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/Applications/ilastik-1.2.0rc10-OSX.app/Contents/ilastik-release/run_ilastik.sh --headless --project=ilastikClassifierFear199.ilp --output_format=tiff \"../rawFear199ITK/runtime/RAWoutfileFear199.niiITK100*.tiff\" \"../rawFear199ITK/runtime/RAWoutfileFear199.niiITK101*.tiff\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the process for the histogram equalized brains, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future goals:\n",
    "\n",
    "Eventually, we want our methodology to do something similar to this: http://ilastik.org/documentation/counting/counting.  We want to be able to use our pixel-based classifier to try to find the number of neurons in a given 3D image.\n",
    "\n",
    "However, much to my chagrin, this methodology currently does NOT support 3D data -- it only supports 2D data.  I can apply the technique to 2D \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
